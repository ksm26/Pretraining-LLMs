# ğŸ§  [Pretraining LLMs](https://www.deeplearning.ai/short-courses/pretraining-llms/)

Welcome to the "Pretraining LLMs" course! ğŸ§‘â€ğŸ« The course dives into the essential steps of pretraining large language models (LLMs).

## ğŸ“˜ Course Summary
In this course, youâ€™ll explore pretraining, the foundational step in training LLMs, which involves teaching an LLM to predict the next token using vast text datasets. 

ğŸ§  You'll learn the essential steps to pretrain an LLM, understand the associated costs, and discover cost-effective methods by leveraging smaller, existing open-source models.

**Detailed Learning Outcomes:**
1. ğŸ§  **Pretraining Basics**: Understand the scenarios where pretraining is the optimal choice for model performance. Compare text generation across different versions of the same model to grasp the performance differences between base, fine-tuned, and specialized pre-trained models.
2. ğŸ—ƒï¸ **Creating High-Quality Datasets**: Learn how to create and clean a high-quality training dataset using web text and existing datasets, and how to package this data for use with the Hugging Face library.
3. ğŸ”§ **Model Configuration**: Explore ways to configure and initialize a model for training, including modifying Metaâ€™s Llama models and initializing weights either randomly or from other models.
4. ğŸš€ **Executing Training Runs**: Learn how to configure and execute a training run to train your own model effectively.
5. ğŸ“Š **Performance Assessment**: Assess your trained modelâ€™s performance and explore common evaluation strategies for LLMs, including benchmark tasks used to compare different modelsâ€™ performance.

## ğŸ”‘ Key Points
- ğŸ§© **Pretraining Process**: Gain in-depth knowledge of the steps to pretrain an LLM, from data preparation to model configuration and performance assessment.
- ğŸ—ï¸ **Model Architecture Configuration**: Explore various options for configuring your modelâ€™s architecture, including modifying Metaâ€™s Llama models and innovative pretraining techniques like Depth Upscaling, which can reduce training costs by up to 70%.
- ğŸ› ï¸ **Practical Implementation**: Learn how to pretrain a model from scratch and continue the pretraining process on your own data using existing pre-trained models.

## ğŸ‘©â€ğŸ« About the Instructors
- ğŸ‘¨â€ğŸ« **Sung Kim**: CEO of Upstage, bringing extensive expertise in LLM pretraining and optimization.
- ğŸ‘©â€ğŸ”¬ **Lucy Park**: Chief Scientific Officer of Upstage, with a deep background in scientific research and LLM development.

ğŸ”— To enroll in the course or for further information, visit ğŸ“š [deeplearning.ai](https://www.deeplearning.ai/short-courses/).
